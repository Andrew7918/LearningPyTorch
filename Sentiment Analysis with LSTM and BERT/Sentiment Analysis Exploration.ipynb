{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Exploration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6DYPhSSX96J"
      },
      "source": [
        "This notebook was originally created from google colab. It will be used for me to experiment with and implement all the steps necessary to do sentiment analysis with BERT. In each stage, I will try to implement my own version of functions/classes in order to practice my understanding, and I'll then implement it over using TorchText version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j45BjA_sfCHs"
      },
      "source": [
        "Note that the functions and classes are used for a sentiment analysis task, namely assuming data is textual input and output is numerical input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlbSAxelZ1Mv"
      },
      "source": [
        "The dataset that I'll use for this notebook comes from : https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
        "\n",
        "It's a small, simple dataset perfect to test whether code works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWNGJkLUW6cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13aec91f-89ff-4487-a600-435c63cddcf3"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy \n",
        "import torch\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PPDopnJ1aDWy",
        "outputId": "18ed06ba-1c0f-4539-89ce-0de1393223a3"
      },
      "source": [
        "'''\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "'''"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom google.colab import files\\nfiles.upload()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEeRoW4tZy7m"
      },
      "source": [
        "## 1. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xc52big0C83"
      },
      "source": [
        "For a short but great tutorial on building custom functions for dataset building, I recommend watching this educational video : https://www.youtube.com/watch?v=9sHcLvVXsns&t=589s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUYQxEyEe_10"
      },
      "source": [
        "### 1a. Preliminary setup\n",
        "\n",
        "Setup preliminary stuff here, like contractions list for text preprocessing, paths, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_caEphE9baLP"
      },
      "source": [
        "root_path = \"/content\"\n",
        "file_name = \"/Tweets.csv\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muCTc2wNzEex"
      },
      "source": [
        "df = pd.read_csv(root_path + file_name)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyx_3bl_XYO4"
      },
      "source": [
        "label_mapping = {\n",
        "    \"neutral\":0,\n",
        "    \"positive\":1,\n",
        "    \"negative\":-1\n",
        "}\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImxepVKLD6ay"
      },
      "source": [
        "# Manually set which column contains label and which column contains text.\n",
        "label_column = 'airline_sentiment'\n",
        "text_column = 'text'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ALvJKtzyNMo"
      },
      "source": [
        "# contractions dictionary\n",
        "contraction_dictionary = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall\",\n",
        "\"he'll've\": \"he shall have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i shall have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall\",\n",
        "\"it'll've\": \"it shall have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she shall have\",\n",
        "\"she's\": \"she has\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they shall have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "'n': 'and'\n",
        "}\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIiVzjaEfMJI"
      },
      "source": [
        "### 1b. Text Preprocessor setup\n",
        "\n",
        "Before text can be used, we need to preprocess it. For that we'll define a custom preprocessor class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-NZxSXcipG"
      },
      "source": [
        "class TextPreprocessor():\n",
        "  '''\n",
        "  This class is used for text preprocessing. The fit_transform method also helps map labels into unique values based on label mapping provided.\n",
        "  '''\n",
        "  def __init__(self, \n",
        "               lower = True, \n",
        "               contraction = True, \n",
        "               contraction_dictionary = None, \n",
        "               punctuation = True,\n",
        "               stop_words = True, \n",
        "               lemmatize = False,\n",
        "               short_words = True, \n",
        "               emojis = True,\n",
        "               alphabet_only = False\n",
        "               ):\n",
        "    self.lower_case = lower\n",
        "    self.contraction = contraction\n",
        "    self.contraction_dictionary = contraction_dictionary\n",
        "    self.punctuation = punctuation \n",
        "    self.stop_words = stop_words\n",
        "    self.lemmatize = lemmatize\n",
        "    self.short_words = short_words\n",
        "    self.emojis = emojis\n",
        "    self.alphabet_only = alphabet_only\n",
        "\n",
        "  def fit_transform(self,data,labels,label_mapping):\n",
        "    '''\n",
        "    For now, the preprocessing takes place step by step and in each step a sentence is returned. Might change this later\n",
        "    so that after the first step or so we get a list of tokens rather than having to join sentence over and over.\n",
        "\n",
        "    Parameters:\n",
        "    data - pandas series, column within dataframe containing text\n",
        "    labels - pandas series, column within dataframe containing target\n",
        "\n",
        "    Returns:\n",
        "    tuple of (data,label) that has been preprocessed. In the end text with no words will be removed.\n",
        "    At this stage, data and labels are still in the form of numpy array.\n",
        "    '''\n",
        "    # Preprocess labels (To be implemented)\n",
        "    labels = labels.apply(lambda s : label_mapping[s])\n",
        "    # Preprocess data\n",
        "    data = data.astype(str)\n",
        "\n",
        "    if self.lower_case == True:\n",
        "      data = data.apply(lambda x : x.lower())\n",
        "    \n",
        "    if self.emojis == True:\n",
        "      data = data.apply(self.remove_emojis)\n",
        "\n",
        "    if self.contraction == True:\n",
        "      assert self.contraction_dictionary is not None, \"self.contraction is True, but no contraction_dictionary was provided.\"\n",
        "      data = data.apply(self.expand_contractions)\n",
        "\n",
        "    if self.punctuation == True:\n",
        "      data = data.apply(self.remove_punctuation)\n",
        "    \n",
        "    if self.alphabet_only == True:\n",
        "      data=data.apply(lambda s: re.sub(r\"[^a-zA-Z]\",\" \",s)) #keep only alphabetical words\n",
        "\n",
        "    if self.lemmatize == True:\n",
        "      data = data.apply(self.lemma)\n",
        "\n",
        "    if self.stop_words == True:\n",
        "      data = data.apply(self.remove_stop_words)\n",
        "\n",
        "    if self.short_words == True:\n",
        "      data = data.apply(self.remove_short_words)\n",
        "    \n",
        "    # Drop the empty entries with empty strings\n",
        "    drop = np.where(data.apply(lambda x : len(x) == 0) == True)\n",
        "    drop = np.add(drop[0],1)\n",
        "    data = data.drop(drop)\n",
        "    labels = labels.drop(drop)\n",
        "    \n",
        "    return (drop, data.values,labels.values)\n",
        "\n",
        "  def transform_text(self,text):\n",
        "    '''\n",
        "    Similar to fit_transform, except this will be used to preprocess and transform any string.\n",
        "\n",
        "    Parameters:\n",
        "    text - string to be transformed\n",
        "    \n",
        "    Returns:\n",
        "    out - transformed string\n",
        "    '''\n",
        "    out = text\n",
        "    if self.lower_case == True:\n",
        "      out = out.lower()\n",
        "    \n",
        "    if self.emojis == True:\n",
        "      out = self.remove_emojis(out)\n",
        "\n",
        "    if self.contraction == True:\n",
        "      assert self.contraction_dictionary is not None, \"self.contraction is True, but no contraction_dictionary was provided.\"\n",
        "      out = self.expand_contractions(out)\n",
        "\n",
        "    if self.punctuation == True:\n",
        "      out = self.remove_punctuation(out)\n",
        "    \n",
        "    if self.alphabet_only == True:\n",
        "      out= re.sub(r\"[^a-zA-Z]\",\" \",out) #keep only alphabetical words\n",
        "\n",
        "    if self.lemmatize == True:\n",
        "      out = self.lemma(out)\n",
        "\n",
        "    if self.stop_words == True:\n",
        "      out = self.remove_stop_words(out)\n",
        "\n",
        "    if self.short_words == True:\n",
        "      out = self.remove_short_words(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_emojis(text):\n",
        "    '''\n",
        "    Returns text with emojis removed.\n",
        "    '''\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                            \"]+\", flags = re.UNICODE)\n",
        "    text=regrex_pattern.sub(r'',text)\n",
        "    text=text.replace('\\n',' ')\n",
        "    out=re.sub(' +', ' ', text)\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def expand_contractions(self,text):\n",
        "    '''\n",
        "    Returns text with contractions expanded.\n",
        "    '''\n",
        "    out=re.sub(r\"â€™\",\"'\",text)\n",
        "    out=out.split(\" \")\n",
        "\n",
        "    for idx,word in enumerate(out):\n",
        "        if word in self.contraction_dictionary:\n",
        "            \n",
        "            out[idx] = self.contraction_dictionary[word]\n",
        "    return \" \".join(out)\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_punctuation(text):\n",
        "    '''\n",
        "    Returns text with punctuations removed.\n",
        "    '''\n",
        "    out = re.sub(r\"[^\\w\\s]\",\" \",text)\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_stop_words(text):\n",
        "    '''\n",
        "    Returns text with stop_words removed.\n",
        "    '''\n",
        "    stop_words = stopwords.words('english')\n",
        "    out=[i for i in text.split(\" \") if i not in stop_words]\n",
        "    return \" \".join(out)\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_short_words(text):\n",
        "    '''\n",
        "    Returns text with short words removed.\n",
        "    '''\n",
        "    out=[i for i in text.split(\" \") if len(i)>2]\n",
        "    return \" \".join(out)\n",
        "\n",
        "  @staticmethod\n",
        "  def lemma(text):\n",
        "    print(\"Lemmatization not implemented yet\")\n",
        "    "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mV4tUtb8yoZ"
      },
      "source": [
        "**Testing to see whether TextPreprocessor works**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvLj-FbdwLsd"
      },
      "source": [
        "# Testing whether the class works\n",
        "preprocessor = TextPreprocessor(contraction_dictionary = contraction_dictionary)\n",
        "dropped_indexes,test_data, test_label = preprocessor.fit_transform(df['text'],df['airline_sentiment'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNVvEw6pxbPv",
        "outputId": "a6b26a9b-1e7a-4f0d-9078-9460d3da2dda"
      },
      "source": [
        "test_data[:15]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['virginamerica dhepburn said',\n",
              "       'virginamerica plus added commercials experience tacky',\n",
              "       'virginamerica today must mean need take another trip',\n",
              "       'virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse',\n",
              "       'virginamerica really big bad thing',\n",
              "       'virginamerica seriously would pay flight seats playing really bad thing flying',\n",
              "       'virginamerica yes nearly every time fly ear worm away',\n",
              "       'virginamerica really missed prime opportunity men without hats parody https mwpg7grezp',\n",
              "       'virginamerica well',\n",
              "       'virginamerica amazing arrived hour early good',\n",
              "       'virginamerica know suicide second leading death among teens',\n",
              "       'virginamerica pretty graphics much better minimal iconography',\n",
              "       'virginamerica great deal already thinking 2nd trip australia amp even gone 1st trip yet',\n",
              "       'virginamerica virginmedia flying fabulous seductive skies take stress away travel http ahlxhhkiyn',\n",
              "       'virginamerica thanks'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDd960eufjGn"
      },
      "source": [
        "### 1c. Vocabulary setup\n",
        "\n",
        "Next we need to define vocabulary object. This helps build mappings between text and numeric indexes. Text can't be fed directly into models and needs to be numericalized, so our vocab object will help us do that. \n",
        "\n",
        "In this block we implement a simple CustomVocab class that maps text to numeric indexes. For vocab object that can do vectorized mappings can use TorchText's one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_EuY9epjw3w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d36ec7c5-c0b2-49d2-f819-a8b26cbdf70a"
      },
      "source": [
        "'''\n",
        "df['review']=df['review'].apply(nltk_lemmatize)\n",
        "df['review']=df['review'].apply(spacy_lemmatize)\n",
        "df['review']=df['review'].apply(lambda s: s.replace('-PRON-',\"\"))\n",
        "'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndf[\\'review\\']=df[\\'review\\'].apply(nltk_lemmatize)\\ndf[\\'review\\']=df[\\'review\\'].apply(spacy_lemmatize)\\ndf[\\'review\\']=df[\\'review\\'].apply(lambda s: s.replace(\\'-PRON-\\',\"\"))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gd9W5oSchIA"
      },
      "source": [
        "class CustomVocabulary():\n",
        "\n",
        "  def __init__(self,preprocessor,freq_threshold = 5):\n",
        "    self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\":0,\"<SOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
        "    self.threshold = freq_threshold\n",
        "    self.preprocessor = preprocessor\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "  \n",
        "  @staticmethod\n",
        "  def tokenize(text):\n",
        "    return text.split(\" \")\n",
        "  \n",
        "  def build_vocab(self,sentence_list):\n",
        "    '''\n",
        "    Build vocab from sentence. the itos entries correspond to words with highest frequencies to words with \n",
        "    lowest frequencies (aparts from special tokens)\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for sentence in sentence_list:\n",
        "      counter.update(self.tokenize(sentence))\n",
        "    \n",
        "    # Sort based on alphabet\n",
        "    sorted_counter = sorted(counter.items(),key = lambda t : t[0])\n",
        "    # Then sort again based on freq\n",
        "    sorted_counter.sort(key = lambda t : t[1],reverse = True)\n",
        "    # This gives list of tuples sorted based on frequencies, then sorted based on alphabetical order\n",
        "\n",
        "    # start updating from index 4 onwards\n",
        "    index = 4\n",
        "    for item in sorted_counter:\n",
        "      word,count = item\n",
        "      if count >= self.threshold:\n",
        "        self.itos[index] = word\n",
        "        self.stoi[word] = index\n",
        "        index += 1\n",
        "  \n",
        "  def numericalize(self,text):\n",
        "    '''\n",
        "    Converts any text into a list of numbers based on stoi. Does not have capabilities for vectorized text.\n",
        "    The text will be preprocessed using self.preprocessor before checked against stoi.\n",
        "\n",
        "    Parameters:\n",
        "    text - string to be numericalized\n",
        "\n",
        "    Returns:\n",
        "    tokens - list of indexes from text based on stoi\n",
        "    '''\n",
        "    preprocessed_text = self.preprocessor.transform_text(text)\n",
        "    tokenized_text = self.tokenize(preprocessed_text)\n",
        "\n",
        "\n",
        "    return [self.stoi[text] if text in self.stoi else self.stoi[\"<UNK>\"] for text in tokenized_text]\n",
        "\n",
        "  def reverse_numericalize(self,text):\n",
        "    '''\n",
        "    Converts list of numericalized text back into text string\n",
        "    '''\n",
        "    return \" \".join([self.itos[t] for t in text])\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koa7aneP-xtG"
      },
      "source": [
        "**Test whether CustomVocabulary works**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQsL_ep75Vvc",
        "outputId": "30dad17a-bdfe-460e-fed6-09f9e5dea04f"
      },
      "source": [
        "vocab = CustomVocabulary(preprocessor)\n",
        "vocab.build_vocab(test_data.tolist())\n",
        "sample = df.sample(1)['text'].values[0]\n",
        "print(f\"original sentence : {sample}\")\n",
        "numericalized_sample = vocab.numericalize(sample)\n",
        "print(f\"numericalized sentence : {numericalized_sample}\")\n",
        "print(f\"reverse numericalised sentence : {vocab.reverse_numericalize(numericalized_sample)}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence : @AmericanAir for my delay and you know what I get a we don't credit anybody back a supervisor who cut me off when speaking\n",
            "numericalized sentence : [7, 64, 47, 10, 197, 3, 31, 546, 916, 1075]\n",
            "reverse numericalised sentence : americanair delay know get credit <UNK> back supervisor cut speaking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6evtoXTLe2AK"
      },
      "source": [
        "### 1d. Dataset setup\n",
        "\n",
        "Using the preprocessor and vocab objects setup, we set up the dataset class. As the name implies, this class is highly necessary in every pytorch-related project since you train the model on datasets. The dataset class basically takes the raw csv, and outputs an object that yields tensors of input data (so in this case numericalized sentence) and target values(labels).\n",
        "\n",
        "To clarify further, input data refers to \"x_values\" and target values refer to \"y_values\" that we feed to the ML/DL model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h45sWyeaaV6r"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "  # preprocesser object will be passed to the dataset, along with the vocab object?\n",
        "  # also assume the dataset has been slightly preprocessed before here. So no duplicates, no nulls hopefully\n",
        "  def __init__(self,root,file_name,label_column,data_column,preprocessor,vocabulary,label_mapping):\n",
        "    self.raw_df = pd.read_csv(root + file_name)\n",
        "    self.vocab = vocabulary\n",
        "    self.preprocessor = preprocessor\n",
        "    self.label_mapping = label_mapping\n",
        "    # preprocessor has a simple fit function that will return preprocessed data and labels\n",
        "    # self.raw_df will remain as raw df\n",
        "    if self.preprocessor is not None:\n",
        "      # data and label will be in the form of numpy array after fit_transform.\n",
        "      self.dropped_indexes, data,label = self.preprocessor.fit_transform(df[data_column],df[label_column],self.label_mapping)\n",
        "    else:\n",
        "      data = self.df[data_column].values\n",
        "      label = self.df[label_column].values\n",
        "      self.dropped_indexes = None\n",
        "    \n",
        "    # pass in list of strings to build vocabulary\n",
        "    self.vocab.build_vocab(data.tolist())\n",
        "    self.data = data\n",
        "    self.label = label\n",
        "  \n",
        "  def __len__(self):\n",
        "    '''\n",
        "    Note that we take length of preprocessed data, so it is possible \n",
        "    the length that gets returned is less than the raw df length.\n",
        "    '''\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    '''\n",
        "    Data will be converted into the corresponding indexes based on self.vocab.\n",
        "\n",
        "    returns tensors of numericalized_sentence,labels\n",
        "    '''\n",
        "    sentence = self.data[index]\n",
        "    label = self.label[index]\n",
        "\n",
        "    numericalized_sentence = [self.vocab.stoi[\"<SOS>\"]]\n",
        "    numericalized_sentence.extend(self.vocab.numericalize(sentence))\n",
        "    numericalized_sentence.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return torch.from_numpy(np.array(numericalized_sentence)),torch.from_numpy(np.array(label))\n",
        "  \n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVWVhnalYfdO"
      },
      "source": [
        "**Testing dataset class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEcwITmtYiDg"
      },
      "source": [
        "dataset = TweetsDataset(root_path,file_name,\"airline_sentiment\",\"text\",preprocessor,vocab,label_mapping)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc4XCVGffsNk"
      },
      "source": [
        "### 1e. DataLoader Setup\n",
        "\n",
        "After setting up dataset, last is to setup a  DataLoader, which allows us to specify how we want to retrieve data from dataset. We also define a custom collate function as well to enable us to do batch padding. \n",
        "\n",
        "This code block basically contains the \"main()\" function for data preparation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-CRXzIdZx0Q"
      },
      "source": [
        "class Collate():\n",
        "  def __init__(self,pad_idx):\n",
        "    '''\n",
        "    pad_idx - index used to represent <PAD>\n",
        "    '''\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self,batch):\n",
        "    '''\n",
        "    Class is called when loading into dataloader. So when preparing batches will go through collate function.\n",
        "    Note that batch is list of (numericalized_sentence,label), where numericalized_sentence is a tensor, label is a tensor\n",
        "\n",
        "    Returns (text,labels), where text.size() = (batch_size,padded_size). labels.size() = batch_size\n",
        "    '''\n",
        "    numericalized_sentence_list = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    # batch_first makes it so the dimension is (batch,longest seq dim, *) where * is other dims\n",
        "    text = pad_sequence(numericalized_sentence_list,batch_first=True,padding_value = self.pad_idx)\n",
        "    labels = torch.from_numpy(np.array(labels))\n",
        "    return text,labels\n",
        "  "
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBnBEVDWCVbW"
      },
      "source": [
        "def prepare_loader(\n",
        "    root_path,\n",
        "    file_name,\n",
        "    data_col,\n",
        "    target_col,\n",
        "    batch_size,\n",
        "    preprocessor,\n",
        "    vocab,\n",
        "    label_mapping,\n",
        "    num_workers = -1,\n",
        "    shuffle = True,\n",
        "    pin_memory = True\n",
        "    ):\n",
        "  '''\n",
        "  Prepares the data loader. It will\n",
        "  1. Prepare dataset (using preprocessor)\n",
        "  2. Load dataset into DataLoader, which will be used to batch up the data during runtime\n",
        "  '''\n",
        "\n",
        "  dataset = TweetsDataset(root_path,file_name,target_col,data_col,preprocessor,vocab,label_mapping)\n",
        "  pad_idx = dataset.vocab.stoi['<PAD>']\n",
        "  loader = DataLoader(dataset = dataset, \n",
        "                      batch_size = batch_size,\n",
        "                      shuffle =shuffle,\n",
        "                      num_workers = num_workers,\n",
        "                      pin_memory = pin_memory,\n",
        "                      collate_fn = Collate(pad_idx)\n",
        "                      )\n",
        "  return loader"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaqTkcTiomZ"
      },
      "source": [
        "**Testing whether dataloader works**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DANl4vjxFxna"
      },
      "source": [
        "preprocessor = TextPreprocessor(contraction_dictionary = contraction_dictionary)\n",
        "vocab = CustomVocabulary(preprocessor)\n",
        "dataloader = prepare_loader(root_path = root_path,\n",
        "                            file_name = file_name,\n",
        "                            data_col = text_column,\n",
        "                            target_col = label_column,\n",
        "                            batch_size = 32,\n",
        "                            preprocessor = preprocessor,\n",
        "                            vocab = vocab,\n",
        "                            label_mapping = label_mapping,\n",
        "                            num_workers = 1,\n",
        "                            shuffle = True,\n",
        "                            pin_memory = True\n",
        "                            )\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQLj2C14etgP"
      },
      "source": [
        "**Testing whether dataloader works**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQWU1NeBGXYF",
        "outputId": "69f52cb2-f143-4f29-d79b-38fb152fbcdc"
      },
      "source": [
        "for i, batch in enumerate(dataloader):\n",
        "  print(batch[0].size())\n",
        "  print(batch[1].size())\n",
        "  # observe 4th batch and stop.\n",
        "  if i == 2:\n",
        "    break"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 18])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 17])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 16])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OgTC_DfehBq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}